{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric optimization\n",
    "\n",
    "To find the optimal weights of the logistic regression, we can use {prf:ref}`gradient descent <GD>` algorithm. To apply this algorithm, one need to calculate the gradient of the loss function.\n",
    "\n",
    "## Binary logistic regression\n",
    "\n",
    "Multiply the loss function {eq}`bin-log-reg-loss` by $n$:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol w) = \n",
    "-\\sum\\limits_{i=1}^n \\big(y_i \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) + (1- y_i)\\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\big).\n",
    "$$\n",
    "\n",
    "To find $\\nabla \\mathcal L(\\boldsymbol w)$ observe that\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = \\frac {\\nabla \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = -\\frac {\\nabla  \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)$?\n",
    "\n",
    "Putting it altogeter, we get\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal L(\\boldsymbol w) = -\\sum\\limits_{i=1}^n \\big(y_i(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\boldsymbol x_i - (1-y_i)\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)\\boldsymbol x_i\\big) = \\sum\\limits_{i=1}^n (\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w) - y_i)\\boldsymbol x_i.\n",
    "$$\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "How to write $\\nabla \\mathcal L(\\boldsymbol w)$ as a product of a matrix and a vector, avoiding the explicit summation?\n",
    "\n",
    "```{hint}\n",
    ":class: dropdown\n",
    "The shape of $\\nabla \\mathcal L(\\boldsymbol w)$ is the same as of $\\boldsymbol w$, i.e., $d\\times 1$. Now observe that\n",
    "\n",
    "$$\n",
    "   \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^\\top \\boldsymbol w) - y_1 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\sigma(\\boldsymbol x_n^\\top \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "   = \\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y \\in \\mathbb R^n.\n",
    "$$\n",
    "\n",
    "What should we multiply by this vector to obtain $\\nabla \\mathcal L$?\n",
    "```\n",
    "````\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    " What is hessian $\\nabla^2 L(\\boldsymbol w)$?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\nabla^2 L(\\boldsymbol w) = \\boldsymbol X^\\top \\boldsymbol S \\boldsymbol X,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "   \\boldsymbol S = \\mathrm{diag}\\{\\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y\\} = \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^{\\boldsymbol{\\top}} \\boldsymbol w) - y_1  & \\ldots & 0 \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   0 & \\ldots & \\sigma(\\boldsymbol x_n^{\\boldsymbol{\\top}} \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "$$\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset: numeric optimization \n",
    "\n",
    "Fetch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the {prf:ref}`gradient descent <GD>` algorithm to the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic_regression_gd(X, y, C=1, learning_rate=0.01, tol=1e-3, max_iter=10000):\n",
    "    w = np.random.normal(size=X.shape[1])\n",
    "    gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(gradient) <= tol:\n",
    "            return w\n",
    "        w -= learning_rate * gradient\n",
    "        gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    print(\"max_iter exceeded\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regresion on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter exceeded\n",
      "CPU times: user 3min 41s, sys: 4.28 s, total: 3min 45s\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.55589631,  0.23718992,  0.84545289, -0.04572754, -1.01137942,\n",
       "       -0.76857   , -0.10062693, -0.94815667,  2.02352411,  0.76994419,\n",
       "       -0.59054628,  1.72377542, -0.49919351, -0.16342549,  0.00423511,\n",
       "       -0.40294277,  0.16167892, -0.71351214,  0.82439777,  0.2459244 ,\n",
       "        0.97408872, -0.67145159, -0.47996041, -0.02177717,  0.6464571 ,\n",
       "       -0.6683169 , -2.10319567, -0.13087973,  0.77725994, -0.98405297])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time w = logistic_regression_gd(X, y, learning_rate=2e-7, max_iter=10**6)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507908611599297"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(expit(X.dot(w)) > 0.5, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595782073813708\n",
      "0.9595782073813708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.19618643,  0.11466367, -0.07126147, -0.0035311 , -0.16764678,\n",
       "        -0.41404964, -0.67982042, -0.36961679, -0.24345819, -0.02414876,\n",
       "        -0.02432342,  1.21550791,  0.04201857, -0.09650802, -0.01902743,\n",
       "         0.01859326, -0.0370666 , -0.04324479, -0.0436697 ,  0.00885636,\n",
       "         1.28558163, -0.34036341, -0.12412841, -0.02460007, -0.31080579,\n",
       "        -1.14514272, -1.63883171, -0.70811924, -0.73547081, -0.11298509]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(fit_intercept=False, max_iter=5000)\n",
    "log_reg.fit(X, y)\n",
    "print(log_reg.score(X, y))\n",
    "print(accuracy_score(log_reg.predict(X), y))\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9963322749134154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w - log_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "Recall that the loss function in this case is\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathcal L(\\boldsymbol W) = -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\boldsymbol x_i^\\top\\boldsymbol w_{k} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp(\\boldsymbol x_i^\\top\\boldsymbol w_{k})\\Big)\\bigg) = \\\\\n",
    "    =\n",
    "    -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\sum\\limits_{j=1}^d x_{ij} w_{jk} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg)\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "One can show that \n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal L(\\boldsymbol W) = \\boldsymbol X^\\top (\\boldsymbol {\\widehat Y} - \\boldsymbol Y) = \\boldsymbol X^\\top ( \\sigma(\\boldsymbol{XW}) - \\boldsymbol Y).\n",
    "$$\n",
    "\n",
    "<!-- Observe that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial w_{pq}} (x_{ij} w_{jk}) = x_{ij} \\delta_{pj} \\delta_{qk},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{pq}}\\bigg(\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg) = \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial w_{pq}} = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K y_{ik}\\bigg(\\sum\\limits_{j=1}^d  \\bigg(  x_{ij} \\delta_{pj} \\delta_{qk} - \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\\bigg)\\bigg)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} TODO\n",
    ":class: warning\n",
    "* Try numerical optimization on several datasets\n",
    "* Apply Newton's method and compare it's performance with GD\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}